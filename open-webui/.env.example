# =============================================================================
# Open WebUI Configuration
# Chat interface for Ollama with RAG capabilities
# =============================================================================

# -----------------------------------------------------------------------------
# Ollama Connection (REQUIRED - Pre-configured for local setup)
# -----------------------------------------------------------------------------
# Connects to Ollama running on macOS host (for GPU acceleration)
OLLAMA_BASE_URL=http://host.docker.internal:11434

# -----------------------------------------------------------------------------
# Web Search via SearXNG (REQUIRED - Pre-configured)
# -----------------------------------------------------------------------------
# Enable RAG web search for enhanced responses
ENABLE_RAG_WEB_SEARCH=true
RAG_WEB_SEARCH_ENGINE=searxng
RAG_WEB_SEARCH_RESULT_COUNT=5
RAG_WEB_SEARCH_CONCURRENT_REQUESTS=10
SEARXNG_QUERY_URL=http://searxng:8080/search?q=<query>

# -----------------------------------------------------------------------------
# Vector Database Integration (OPTIONAL)
# -----------------------------------------------------------------------------
# Enable Qdrant for advanced RAG capabilities
# Uncomment to use Qdrant vector database for document embeddings
# ENABLE_RAG_LOCAL_WEB_FETCH=true
# VECTOR_DB=qdrant
# QDRANT_URL=http://qdrant:6333

# -----------------------------------------------------------------------------
# Authentication (REQUIRED - Pre-configured for local-only)
# -----------------------------------------------------------------------------
# Authentication disabled for local-only setup
# Change to 'true' if you need password protection
WEBUI_AUTH=false

# -----------------------------------------------------------------------------
# Service Information
# -----------------------------------------------------------------------------
# Access URL: http://chat.localhost
#
# Features:
# - Chat with local Ollama models (gpt-oss, llama3.1)
# - Web search integration via SearXNG
# - Optional RAG with Qdrant for document Q&A
# - No authentication required (local network only)
#
# -----------------------------------------------------------------------------
# Important: Ollama Context Window Configuration
# -----------------------------------------------------------------------------
# By default, Ollama models use a 2048 token context window (~1500 words)
# This can be limiting for long conversations or document analysis
#
# Recommended: Increase context to 32k-128k tokens in Open WebUI settings:
#
# To configure in Open WebUI:
#   1. Go to Settings â†’ Models
#   2. Select your model (e.g., llama3.1:8b)
#   3. Click 'Advanced Parameters'
#   4. Set 'Context Length' to:
#      - 32768 (32k)  - Good for most use cases
#      - 65536 (64k)  - Ideal for long conversations
#      - 131072 (128k) - Maximum for llama3.1/gpt-oss
#   5. Click 'Save'
#
# Or create a custom model via Ollama CLI:
#   echo 'FROM llama3.1:8b' > /tmp/modelfile
#   echo 'PARAMETER num_ctx 32768' >> /tmp/modelfile
#   ollama create llama3.1-32k -f /tmp/modelfile
#
# Note: Higher context uses more VRAM (~1GB per 4k tokens)
# Start with 32k and increase if needed
